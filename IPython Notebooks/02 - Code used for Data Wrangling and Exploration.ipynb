{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = \"../raleigh_north-carolina.osm\"\n",
    "#datafile = \"../Submission/04-sample.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit Data for some fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "        # YOUR CODE HERE\n",
    "        tags = {}\n",
    "        for event, elem in ET.iterparse(filename):\n",
    "            if event == 'end':\n",
    "                if elem.tag not in tags.keys():\n",
    "                    tags[elem.tag] = 1\n",
    "                else:\n",
    "                    tags[elem.tag] += 1\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'way': 211467, 'tag': 813843, 'node': 2524263, 'osm': 1, 'nd': 2784745, 'member': 7647, 'bounds': 1, 'relation': 732}\n",
      "Total Nodes and Ways = 2,735,730\n"
     ]
    }
   ],
   "source": [
    "tags = count_tags(datafile)\n",
    "print(tags)\n",
    "print('Total Nodes and Ways = {:,}'.format(tags['node']+tags['way']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to audit specific kinds of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator function to yield specific tags\n",
    "def audit(osmfile, tagname):\n",
    "    with open(osmfile, \"r\") as osm_file:\n",
    "        for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "            if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "                for tag in elem.iter(\"tag\"):\n",
    "                    if tag.attrib['k'] == tagname:\n",
    "                        yield tag.attrib['v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highway {'Wake Forest Highway', 'Apex Highway'}\n",
      "Grove {'Newton Grove'}\n",
      "Ext {'New Hope Commons Boulevard Ext'}\n",
      "100 {'100'}\n",
      "Extension {'Weaver Dairy Road Extension'}\n",
      "Bypass {'US 15 501 Bypass'}\n",
      "Suite {'N Duke St Suite'}\n",
      "70 {'US 70'}\n",
      "East {'US Highway 70 East'}\n",
      "17 {'US Highway 17'}\n",
      "Practice {'Triangle Family Practice'}\n",
      "Hills {'The Circle at North Hills'}\n",
      "West {'Highway 54 West', 'NC Highway 55 West', 'Highway 55 West', 'Highway West'}\n",
      "PI {'Alexander Promenade PI'}\n",
      "751 {'NC Highway 751'}\n",
      "501 {'US 15;US 501'}\n",
      "55 {'NC Highway 55', 'Highway 55', 'US 55'}\n",
      "54 {'Highway 54', 'West NC Highway 54', 'West Highway 54', 'State Highway 54'}\n",
      "1000 {'Six Forks Road #1000'}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "#Street Names\n",
    "##############\n",
    "\n",
    "imported_street_names = audit(datafile,'addr:street')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "#Known Street Types\n",
    "expected_street_names = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Loop\", \"Way\", \"Run\", \"Circle\", \"Hill\", \"Fork\", \"Plaza\",\n",
    "           \"Point\",\"Terrace\", \"Crescent\", \"Crossing\"]\n",
    "\n",
    "#Known Mappings\n",
    "mapping_street_names = {\"St\": \"Street\", \"St.\": \"Street\", \"St,\":\"Street\", \"ST\":\"Street\", \n",
    "            \"Rd\": \"Road\", \"Rd.\": \"Road\", \"Ave\":\"Avenue\", \"Ave.\":\"Avenue\", \"Blvd\":\"Boulevard\",\n",
    "            \"Blvd.\":\"Boulevard\", \"Pkwy\":\"Parkway\", \"Pky\":\"Parkway\", \"Dr\":\"Drive\", \"Ln\":\"Lane\",\n",
    "            \"Ct\":\"Court\", \"Pl\":\"Place\", \"Cir\":\"Circle\", \"N\":\"North\",\"E\":\"East\",\"S\":\"South\",\"W\":\"West\"}\n",
    "\n",
    "specific_streetname = {\"Meadowmont Village CIrcle\":\"Meadowmont Village Circle\",\n",
    "                       \"LaurelcherryStreet\":\"Laurel Cherry Street\",\n",
    "                      \"Garrett Driver\":\"Garrett Drive\"}\n",
    "\n",
    "def correct_streetname(streetname):\n",
    "    \"\"\"Attempt to correct a street name\"\"\"\n",
    "    if streetname in specific_streetname.keys():\n",
    "        return specific_streetname[streetname]\n",
    "    else:\n",
    "        name = streetname.split(' ')\n",
    "        for idx, subname in enumerate(name):\n",
    "            if subname in mapping_street_names.keys():\n",
    "                name[idx] = mapping_street_names[subname]\n",
    "        return \" \".join(name)\n",
    "\n",
    "def audit_street_type(street_names):\n",
    "    \"\"\"Check if street name is a known name after correction.  If not, record it.\"\"\"\n",
    "    recorded_names = defaultdict(set)\n",
    "    for street_name in street_names:\n",
    "        corrected = correct_streetname(street_name)\n",
    "        m = street_type_re.search(corrected)\n",
    "        if m:\n",
    "            street_type = m.group()           \n",
    "            if street_type not in expected_street_names:\n",
    "                recorded_names[street_type].add(street_name)\n",
    "    return recorded_names\n",
    "            \n",
    "\n",
    "street_types = audit_street_type(imported_street_names)\n",
    "for k,v in street_types.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,564 total validpostal codes found\n",
      "[(27612, 1683), (27609, 1120), (27519, 839), (27701, 659), (27705, 478), (27615, 344), (27510, 267), (27514, 165), (27511, 113), (27606, 98), (27513, 92), (27707, 89), (27601, 84), (27517, 72), (27560, 65), (27704, 54), (27516, 53), (27713, 49), (27703, 48), (27617, 33), (27613, 28), (27603, 20), (27604, 19), (27607, 19), (27610, 15), (27605, 11), (27614, 10), (27608, 9), (27695, 5), (27616, 4), (27162, 4), (27518, 4), (27602, 2), (27599, 2), (27895, 2), (27708, 2), (28616, 1), (27710, 1), (27502, 1)]\n",
      "------------------------------\n",
      "6 invalid postal codes were found\n",
      "{'275198404', '275199', 'NC', '2612-6401', '277030', '275609194'}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "#Postal Codes\n",
    "##############\n",
    "\n",
    "imported_postcodes = audit(datafile,'addr:postcode')\n",
    "postcode_re = re.compile(r'^[0-9]{5}$')\n",
    "extended_postcode_re = re.compile(r'^[0-9]{5}-[0-9]{4}$')\n",
    "\n",
    "def correct_postcode(postcode):\n",
    "    \"\"\"Try to convert postcode to 5 digit int\"\"\"\n",
    "    if extended_postcode_re.match(postcode): #strip extended postcode with \"-####\"\n",
    "        postcode = postcode[0:5]\n",
    "        return int(postcode)\n",
    "    elif postcode_re.match(postcode): #normal 5 digit postcode\n",
    "        return int(postcode)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def audit_postcodes(postcodes):\n",
    "    \"\"\"Try to convert to int.  Record if it doesn't work.\"\"\"\n",
    "    valid = Counter()\n",
    "    invalid = set()\n",
    "    for postcode in postcodes:\n",
    "        postcode_fix = correct_postcode(postcode)\n",
    "        if postcode_fix:\n",
    "            valid[postcode_fix] += 1\n",
    "        else:\n",
    "            invalid.add(postcode)\n",
    "        \n",
    "    return valid, invalid\n",
    "\n",
    "valid_postcode, invalid_postcode = audit_postcodes(imported_postcodes)\n",
    "\n",
    "print(\"{:,} total validpostal codes found\".format(sum(valid_postcode.values())))\n",
    "print(valid_postcode.most_common())\n",
    "\n",
    "print('-'*30)\n",
    "\n",
    "print(\"{} invalid postal codes were found\".format(len(invalid_postcode)))\n",
    "print(invalid_postcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct and Convert to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Function for Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_node(element):\n",
    "    \"\"\"Format Node object and associated tags.  Remove attributes as they are processed.\"\"\"\n",
    "    node = {'type':'node'}\n",
    "    #Extract raw information for this node\n",
    "    attributes = {a:element.attrib[a] for a in element.attrib}\n",
    "    tags = [(t.attrib['k'],t.attrib['v']) for t in element.findall('tag')]\n",
    "    #tags used a list of tuples instead of dict because of potential duplicate keys\n",
    "    \n",
    "    #Add ID\n",
    "    node['id'] = element.attrib['id']\n",
    "    del attributes['id']\n",
    "    \n",
    "    #Add Position\n",
    "    if 'lat' in attributes and 'lon' in attributes:\n",
    "        node['pos'] = [float(element.attrib[\"lat\"]), float(element.attrib[\"lon\"])]\n",
    "        del attributes['lat']\n",
    "        del attributes['lon']\n",
    "\n",
    "    #Created\n",
    "    CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "    created_attrib = [a for a in attributes if a in CREATED] #Which of the created attr are present\n",
    "    if len(created_attrib) > 0:\n",
    "        node['created'] = {}\n",
    "        for a in created_attrib:\n",
    "            node['created'][a] = element.attrib[a]\n",
    "            del attributes[a]\n",
    "    \n",
    "    #Remaining Attributes\n",
    "    for a in attributes:\n",
    "        if problemchars.match(a): #skip attributes with problematic keys\n",
    "            continue\n",
    "        else:\n",
    "            node[a] = element.attrib[a]\n",
    "\n",
    "    #Process tags\n",
    "    problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "    address = re.compile(r'^addr:([a-z]|_)+$')#match 'addr', one colon, some tag\n",
    "    gnis = re.compile(r'^gnis:([a-z]|_)+$')#similar structure to address.  Seems interesting.\n",
    "    lower = re.compile(r'^([a-z]|_)*$')\n",
    "    lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "\n",
    "    for k,v in tags:\n",
    "        if problemchars.match(k):\n",
    "            continue\n",
    "        elif address.match(k):\n",
    "            #Add Address dict if needed\n",
    "            if 'address' not in node.keys(): #add address dict if not present\n",
    "                node['address'] = dict()\n",
    "            #Get subnode and corrected (if possible) value (see above)\n",
    "            subnode = k.split(':')[1]\n",
    "            if subnode == 'street':\n",
    "                v = correct_streetname(v)\n",
    "            elif subnode == 'postcode':\n",
    "                v = correct_postcode(v)\n",
    "            #Add value\n",
    "            node['address'][subnode] = v\n",
    "        elif gnis.match(k):\n",
    "            if 'gnis' not in node.keys(): #add address dict if not present\n",
    "                node['gnis'] = dict()\n",
    "            node['gnis'][k.split(':')[1]] = v\n",
    "        elif lower.match(k) or lower_colon.match(k):\n",
    "            node[k] = v         \n",
    "    \n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting function for Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_way(element):\n",
    "    \"\"\"Format Way object and associated nd and tags\"\"\"\n",
    "    way = {'type':'way'}\n",
    "    \n",
    "    #Extract raw information for this node\n",
    "    attributes = {a:element.attrib[a] for a in element.attrib}\n",
    "    noderefs = [n.attrib['ref'] for n in element.findall('nd')]\n",
    "    tags = [(t.attrib['k'],t.attrib['v']) for t in element.findall('tag')]\n",
    "    \n",
    "    #Add ID\n",
    "    way['id'] = element.attrib['id']\n",
    "    del attributes['id']\n",
    "    \n",
    "    #Created\n",
    "    CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "    created_attrib = [a for a in attributes if a in CREATED] #Which of the created attr are present\n",
    "    if len(created_attrib) > 0:\n",
    "        way['created'] = {}\n",
    "        for a in created_attrib:\n",
    "            way['created'][a] = element.attrib[a]\n",
    "            del attributes[a]\n",
    "            \n",
    "    #Remaining Attributes\n",
    "    for a in attributes:\n",
    "        if problemchars.match(a): #skip attributes with problematic keys\n",
    "            continue\n",
    "        else:\n",
    "            way[a] = element.attrib[a]\n",
    "            \n",
    "    #Node Refs\n",
    "    if len(noderefs) > 0:\n",
    "        way['node_refs'] = noderefs\n",
    "    \n",
    "    #Process tags\n",
    "    problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "    tiger = re.compile(r'^tiger:([a-z]|_)+$') #Import from US Census Data for roadways, etc\n",
    "    lower = re.compile(r'^([a-z]|_)*$')\n",
    "    lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "\n",
    "    for k,v in tags:\n",
    "        if problemchars.match(k):\n",
    "            continue\n",
    "        elif tiger.match(k):\n",
    "            if 'tiger' not in way.keys(): #add address dict if not present\n",
    "                way['tiger'] = dict()\n",
    "            way['tiger'][k.split(':')[1]] = v\n",
    "        elif lower.match(k) or lower_colon.match(k):\n",
    "            way[k] = v \n",
    "    \n",
    "    return way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    if element.tag == \"node\":\n",
    "        shaped_element = process_node(element)\n",
    "    elif element.tag == 'way':\n",
    "        shaped_element = process_way(element)\n",
    "    else:\n",
    "        shaped_element = None\n",
    "    return shaped_element\n",
    "        \n",
    "\n",
    "def process_map(file_in):\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    with open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                fo.write(json.dumps(el)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_map(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-07T00:38:40.359-0400\tconnected to: localhost\n",
      "2015-07-07T00:38:40.359-0400\tdropping: osm.raleigh\n",
      "2015-07-07T00:38:43.334-0400\t[#.......................] osm.raleigh\t25.0 MB/562.6 MB (4.4%)\n",
      "2015-07-07T00:38:46.334-0400\t[##......................] osm.raleigh\t51.5 MB/562.6 MB (9.2%)\n",
      "2015-07-07T00:38:49.334-0400\t[###.....................] osm.raleigh\t79.0 MB/562.6 MB (14.0%)\n",
      "2015-07-07T00:38:52.334-0400\t[####....................] osm.raleigh\t107.4 MB/562.6 MB (19.1%)\n",
      "2015-07-07T00:38:55.334-0400\t[#####...................] osm.raleigh\t135.4 MB/562.6 MB (24.1%)\n",
      "2015-07-07T00:38:58.334-0400\t[######..................] osm.raleigh\t154.4 MB/562.6 MB (27.4%)\n",
      "2015-07-07T00:39:01.334-0400\t[#######.................] osm.raleigh\t180.3 MB/562.6 MB (32.0%)\n",
      "2015-07-07T00:39:04.334-0400\t[########................] osm.raleigh\t207.6 MB/562.6 MB (36.9%)\n",
      "2015-07-07T00:39:07.334-0400\t[#########...............] osm.raleigh\t234.4 MB/562.6 MB (41.7%)\n",
      "2015-07-07T00:39:10.334-0400\t[###########.............] osm.raleigh\t260.9 MB/562.6 MB (46.4%)\n",
      "2015-07-07T00:39:13.334-0400\t[############............] osm.raleigh\t288.5 MB/562.6 MB (51.3%)\n",
      "2015-07-07T00:39:16.335-0400\t[#############...........] osm.raleigh\t316.5 MB/562.6 MB (56.3%)\n",
      "2015-07-07T00:39:19.334-0400\t[##############..........] osm.raleigh\t344.3 MB/562.6 MB (61.2%)\n",
      "2015-07-07T00:39:22.335-0400\t[###############.........] osm.raleigh\t367.7 MB/562.6 MB (65.3%)\n",
      "2015-07-07T00:39:25.338-0400\t[################........] osm.raleigh\t395.1 MB/562.6 MB (70.2%)\n",
      "2015-07-07T00:39:28.334-0400\t[#################.......] osm.raleigh\t414.0 MB/562.6 MB (73.6%)\n",
      "2015-07-07T00:39:31.338-0400\t[##################......] osm.raleigh\t441.8 MB/562.6 MB (78.5%)\n",
      "2015-07-07T00:39:34.334-0400\t[####################....] osm.raleigh\t469.4 MB/562.6 MB (83.4%)\n",
      "2015-07-07T00:39:37.334-0400\t[#####################...] osm.raleigh\t503.9 MB/562.6 MB (89.6%)\n",
      "2015-07-07T00:39:40.334-0400\t[#######################.] osm.raleigh\t540.8 MB/562.6 MB (96.1%)\n",
      "2015-07-07T00:39:42.211-0400\timported 2735730 documents\n"
     ]
    }
   ],
   "source": [
    "#Using mongoimport\n",
    "!mongoimport -d osm -c raleigh --drop --file=\"../raleigh_north-carolina.osm.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Auditing of the imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.osm\n",
    "collection = db.raleigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2735730"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of documents\n",
    "collection.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education amenity tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'Duke University East Campus', 'count': 3}\n",
      "{'_id': 'Duke University Central Campus', 'count': 1}\n",
      "{'_id': 'North Carolina State University (Centennial Campus)', 'count': 1}\n",
      "{'_id': None, 'count': 1}\n",
      "{'_id': 'Duke University West Campus', 'count': 1}\n",
      "{'_id': \"St. Augustine's University\", 'count': 1}\n",
      "{'_id': 'Duke University Medical Center', 'count': 1}\n",
      "{'_id': 'JC Raulston Arboretum at NC State University', 'count': 1}\n",
      "{'_id': 'Campbell University: Norman Adrian Wiggins School of Law', 'count': 1}\n",
      "{'_id': 'North Carolina Central University', 'count': 1}\n"
     ]
    }
   ],
   "source": [
    "#List of university\n",
    "pipeline = [\n",
    "    {'$match':{'amenity':'university'}},\n",
    "    {'$group':{'_id':'$name', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$limit':10}\n",
    "]\n",
    "documents = collection.aggregate(pipeline)\n",
    "for r in documents['result']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': None, 'count': 28}\n",
      "{'_id': 'White Building', 'count': 1}\n",
      "{'_id': 'Collins Building', 'count': 1}\n",
      "{'_id': 'Durham Tech Community College', 'count': 1}\n",
      "{'_id': 'Wake Technical Community College: Perry Health Sciences Campus', 'count': 1}\n",
      "{'_id': 'Meredith College', 'count': 1}\n",
      "{'_id': 'AKG Guitar Lessons', 'count': 1}\n"
     ]
    }
   ],
   "source": [
    "#List of college\n",
    "pipeline = [\n",
    "    {'$match':{'amenity':'college'}},\n",
    "    {'$group':{'_id':'$name', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$limit':10}\n",
    "]\n",
    "documents = collection.aggregate(pipeline)\n",
    "for r in documents['result']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': None, 'count': 21}\n",
      "{'_id': 'Carrboro Elementary School', 'count': 2}\n",
      "{'_id': \"Lowe's Grove Middle School\", 'count': 2}\n",
      "{'_id': 'Durham Technical Community College', 'count': 2}\n",
      "{'_id': 'Durham Academy', 'count': 2}\n",
      "{'_id': 'Eastway Elementary School', 'count': 2}\n",
      "{'_id': 'The Goddard School', 'count': 2}\n",
      "{'_id': 'Ravenscroft School', 'count': 2}\n",
      "{'_id': 'Panther Creek High School', 'count': 2}\n",
      "{'_id': 'C C Spaulding Elementary School', 'count': 2}\n"
     ]
    }
   ],
   "source": [
    "#List of school\n",
    "pipeline = [\n",
    "    {'$match':{'amenity':'school'}},\n",
    "    {'$group':{'_id':'$name', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$limit':10}\n",
    "]\n",
    "documents = collection.aggregate(pipeline)\n",
    "for r in documents['result']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Multiple points on the same position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ \n",
      " [35.8950081, -79.0633303] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '373215082', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '444838', 'timestamp': '2009-04-12T14:28:50Z'}, '_id': ObjectId('559b57d50d0edd8d31948483'), 'pos': [35.8950081, -79.0633303]}\n",
      "{'id': '373216876', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '444838', 'timestamp': '2009-04-12T14:31:27Z'}, '_id': ObjectId('559b57d50d0edd8d31948506'), 'pos': [35.8950081, -79.0633303]}\n",
      "{'id': '373217228', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '444838', 'timestamp': '2009-04-12T14:31:56Z'}, '_id': ObjectId('559b57d50d0edd8d31948546'), 'pos': [35.8950081, -79.0633303]}\n",
      "{'id': '373220087', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '444838', 'timestamp': '2009-04-12T14:36:37Z'}, '_id': ObjectId('559b57d50d0edd8d3194873e'), 'pos': [35.8950081, -79.0633303]}\n",
      "------------------------------ \n",
      " [35.8784081, -78.7838664] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '343671678', 'type': 'node', 'created': {'uid': '2294834', 'timestamp': '2015-06-24T17:49:40Z', 'version': '5', 'changeset': '32188921', 'user': 'bigal945'}, '_id': ObjectId('559b57d40d0edd8d31940ee8'), 'pos': [35.8784081, -78.7838664]}\n",
      "{'id': '3615653571', 'type': 'node', 'created': {'uid': '2294834', 'user': 'bigal945', 'version': '1', 'changeset': '32188921', 'timestamp': '2015-06-24T17:49:40Z'}, '_id': ObjectId('559b58070d0edd8d31b7c6b6'), 'pos': [35.8784081, -78.7838664]}\n",
      "{'id': '3615653572', 'type': 'node', 'created': {'uid': '2294834', 'version': '1', 'user': 'bigal945', 'changeset': '32188921', 'timestamp': '2015-06-24T17:49:40Z'}, '_id': ObjectId('559b58070d0edd8d31b7c6b7'), 'pos': [35.8784081, -78.7838664]}\n",
      "{'id': '3615653573', 'type': 'node', 'created': {'uid': '2294834', 'user': 'bigal945', 'version': '1', 'changeset': '32188921', 'timestamp': '2015-06-24T17:49:40Z'}, '_id': ObjectId('559b58070d0edd8d31b7c6b8'), 'pos': [35.8784081, -78.7838664]}\n",
      "------------------------------ \n",
      " [35.7877479, -78.8708045] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '195496249', 'type': 'node', 'created': {'uid': '1494110', 'version': '7', 'user': 'KristenK', 'changeset': '18183244', 'timestamp': '2013-10-04T18:14:14Z'}, '_id': ObjectId('559b57d30d0edd8d31931043'), 'pos': [35.7877479, -78.8708045]}\n",
      "{'id': '2482640856', 'type': 'node', 'created': {'timestamp': '2013-10-04T18:13:55Z', 'version': '1', 'changeset': '18183244', 'user': 'KristenK', 'uid': '1494110'}, '_id': ObjectId('559b58040d0edd8d31b62145'), 'pos': [35.7877479, -78.8708045]}\n",
      "{'id': '2482640858', 'type': 'node', 'created': {'timestamp': '2013-10-04T18:13:55Z', 'version': '1', 'user': 'KristenK', 'changeset': '18183244', 'uid': '1494110'}, '_id': ObjectId('559b58040d0edd8d31b62146'), 'pos': [35.7877479, -78.8708045]}\n",
      "{'id': '2482640859', 'type': 'node', 'created': {'uid': '1494110', 'user': 'KristenK', 'version': '1', 'changeset': '18183244', 'timestamp': '2013-10-04T18:13:55Z'}, '_id': ObjectId('559b58040d0edd8d31b62147'), 'pos': [35.7877479, -78.8708045]}\n",
      "------------------------------ \n",
      " [35.7652757, -78.6962395] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '433105528', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '1722510', 'timestamp': '2009-07-03T17:21:33Z'}, '_id': ObjectId('559b57d50d0edd8d3194f3d2'), 'pos': [35.7652757, -78.6962395]}\n",
      "{'id': '434033085', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '1731725', 'timestamp': '2009-07-04T19:06:55Z'}, '_id': ObjectId('559b57d60d0edd8d3195b8ac'), 'pos': [35.7652757, -78.6962395]}\n",
      "{'id': '434212172', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '1734573', 'timestamp': '2009-07-05T01:21:26Z'}, '_id': ObjectId('559b57d70d0edd8d3195ee80'), 'pos': [35.7652757, -78.6962395]}\n",
      "------------------------------ \n",
      " [35.7634023, -78.6973579] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '433368425', 'type': 'node', 'created': {'timestamp': '2009-07-03T21:31:27Z', 'version': '1', 'changeset': '1724610', 'user': 'jumbanho', 'uid': '38487'}, '_id': ObjectId('559b57d50d0edd8d3194f974'), 'pos': [35.7634023, -78.6973579]}\n",
      "{'id': '434033067', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '1731725', 'timestamp': '2009-07-04T19:06:55Z'}, '_id': ObjectId('559b57d60d0edd8d3195b8a3'), 'pos': [35.7634023, -78.6973579]}\n",
      "{'id': '434202158', 'type': 'node', 'created': {'uid': '38487', 'user': 'jumbanho', 'version': '1', 'changeset': '1734325', 'timestamp': '2009-07-05T00:46:02Z'}, '_id': ObjectId('559b57d70d0edd8d3195eab9'), 'pos': [35.7634023, -78.6973579]}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {'$group':{'_id':'$pos', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$skip':1}, #skip \"None\" record\n",
    "    {'$limit':3}\n",
    "]\n",
    "documents = collection.aggregate(pipeline, allowDiskUse=True)\n",
    "\n",
    "for r in documents['result']:\n",
    "    pos = r['_id']\n",
    "    print(\"-\"*30,\"\\n\",pos,\"\\n\",\"-\"*30,\"\\n\")\n",
    "    overlaps = collection.aggregate({'$match':{'pos':pos}})\n",
    "    for o in overlaps['result']:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
