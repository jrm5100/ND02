{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = \"../raleigh_north-carolina.osm\"\n",
    "#datafile = \"../Submission/04-sample.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit Data for some fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "        # YOUR CODE HERE\n",
    "        tags = {}\n",
    "        for event, elem in ET.iterparse(filename):\n",
    "            if event == 'end':\n",
    "                if elem.tag not in tags.keys():\n",
    "                    tags[elem.tag] = 1\n",
    "                else:\n",
    "                    tags[elem.tag] += 1\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'osm': 1, 'nd': 2784745, 'tag': 813843, 'relation': 732, 'node': 2524263, 'bounds': 1, 'member': 7647, 'way': 211467}\n",
      "Total Nodes and Ways = 2,735,730\n"
     ]
    }
   ],
   "source": [
    "tags = count_tags(datafile)\n",
    "print(tags)\n",
    "print('Total Nodes and Ways = {:,}'.format(tags['node']+tags['way']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to audit specific kinds of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator function to yield specific tags\n",
    "def audit(osmfile, tagname):\n",
    "    with open(osmfile, \"r\") as osm_file:\n",
    "        for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "            if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "                for tag in elem.iter(\"tag\"):\n",
    "                    if tag.attrib['k'] == tagname:\n",
    "                        yield tag.attrib['v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grove {'Newton Grove'}\n",
      "Practice {'Triangle Family Practice'}\n",
      "751 {'NC Highway 751'}\n",
      "Highway {'Wake Forest Highway', 'Apex Highway'}\n",
      "West {'Highway West', 'Highway 55 West', 'Highway 54 West', 'NC Highway 55 West'}\n",
      "70 {'US 70'}\n",
      "Suite {'N Duke St Suite'}\n",
      "PI {'Alexander Promenade PI'}\n",
      "1000 {'Six Forks Road #1000'}\n",
      "Hills {'The Circle at North Hills'}\n",
      "Bypass {'US 15 501 Bypass'}\n",
      "54 {'West Highway 54', 'Highway 54', 'West NC Highway 54', 'State Highway 54'}\n",
      "Extension {'Weaver Dairy Road Extension'}\n",
      "501 {'US 15;US 501'}\n",
      "17 {'US Highway 17'}\n",
      "100 {'100'}\n",
      "55 {'Highway 55', 'US 55', 'NC Highway 55'}\n",
      "East {'US Highway 70 East'}\n",
      "Ext {'New Hope Commons Boulevard Ext'}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "#Street Names\n",
    "##############\n",
    "\n",
    "imported_street_names = audit(datafile,'addr:street')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "#Known Street Types\n",
    "expected_street_names = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Loop\", \"Way\", \"Run\", \"Circle\", \"Hill\", \"Fork\", \"Plaza\",\n",
    "           \"Point\",\"Terrace\", \"Crescent\", \"Crossing\"]\n",
    "\n",
    "#Known Mappings\n",
    "mapping_street_names = {\"St\": \"Street\", \"St.\": \"Street\", \"St,\":\"Street\", \"ST\":\"Street\", \n",
    "            \"Rd\": \"Road\", \"Rd.\": \"Road\", \"Ave\":\"Avenue\", \"Ave.\":\"Avenue\", \"Blvd\":\"Boulevard\",\n",
    "            \"Blvd.\":\"Boulevard\", \"Pkwy\":\"Parkway\", \"Pky\":\"Parkway\", \"Dr\":\"Drive\", \"Ln\":\"Lane\",\n",
    "            \"Ct\":\"Court\", \"Pl\":\"Place\", \"Cir\":\"Circle\", \"N\":\"North\",\"E\":\"East\",\"S\":\"South\",\"W\":\"West\"}\n",
    "\n",
    "specific_streetname = {\"Meadowmont Village CIrcle\":\"Meadowmont Village Circle\",\n",
    "                       \"LaurelcherryStreet\":\"Laurel Cherry Street\",\n",
    "                      \"Garrett Driver\":\"Garrett Drive\"}\n",
    "\n",
    "def correct_streetname(streetname):\n",
    "    \"\"\"Attempt to correct a street name\"\"\"\n",
    "    if streetname in specific_streetname.keys():\n",
    "        return specific_streetname[streetname]\n",
    "    else:\n",
    "        name = streetname.split(' ')\n",
    "        for idx, subname in enumerate(name):\n",
    "            if subname in mapping_street_names.keys():\n",
    "                name[idx] = mapping_street_names[subname]\n",
    "        return \" \".join(name)\n",
    "\n",
    "def audit_street_type(street_names):\n",
    "    \"\"\"Check if street name is a known name after correction.  If not, record it.\"\"\"\n",
    "    recorded_names = defaultdict(set)\n",
    "    for street_name in street_names:\n",
    "        corrected = correct_streetname(street_name)\n",
    "        m = street_type_re.search(corrected)\n",
    "        if m:\n",
    "            street_type = m.group()           \n",
    "            if street_type not in expected_street_names:\n",
    "                recorded_names[street_type].add(street_name)\n",
    "    return recorded_names\n",
    "            \n",
    "\n",
    "street_types = audit_street_type(imported_street_names)\n",
    "for k,v in street_types.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,564 total validpostal codes found\n",
      "[(27612, 1683), (27609, 1120), (27519, 839), (27701, 659), (27705, 478), (27615, 344), (27510, 267), (27514, 165), (27511, 113), (27606, 98), (27513, 92), (27707, 89), (27601, 84), (27517, 72), (27560, 65), (27704, 54), (27516, 53), (27713, 49), (27703, 48), (27617, 33), (27613, 28), (27603, 20), (27604, 19), (27607, 19), (27610, 15), (27605, 11), (27614, 10), (27608, 9), (27695, 5), (27616, 4), (27162, 4), (27518, 4), (27602, 2), (27599, 2), (27895, 2), (27708, 2), (28616, 1), (27710, 1), (27502, 1)]\n",
      "------------------------------\n",
      "6 invalid postal codes were found\n",
      "{'277030', '275199', '275609194', 'NC', '275198404', '2612-6401'}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "#Postal Codes\n",
    "##############\n",
    "\n",
    "imported_postcodes = audit(datafile,'addr:postcode')\n",
    "postcode_re = re.compile(r'^[0-9]{5}$')\n",
    "extended_postcode_re = re.compile(r'^[0-9]{5}-[0-9]{4}$')\n",
    "\n",
    "def correct_postcode(postcode):\n",
    "    \"\"\"Try to convert postcode to 5 digit int\"\"\"\n",
    "    if extended_postcode_re.match(postcode): #strip extended postcode with \"-####\"\n",
    "        postcode = postcode[0:5]\n",
    "        return int(postcode)\n",
    "    elif postcode_re.match(postcode): #normal 5 digit postcode\n",
    "        return int(postcode)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def audit_postcodes(postcodes):\n",
    "    \"\"\"Try to convert to int.  Record if it doesn't work.\"\"\"\n",
    "    valid = Counter()\n",
    "    invalid = set()\n",
    "    for postcode in postcodes:\n",
    "        postcode_fix = correct_postcode(postcode)\n",
    "        if postcode_fix:\n",
    "            valid[postcode_fix] += 1\n",
    "        else:\n",
    "            invalid.add(postcode)\n",
    "        \n",
    "    return valid, invalid\n",
    "\n",
    "valid_postcode, invalid_postcode = audit_postcodes(imported_postcodes)\n",
    "\n",
    "print(\"{:,} total validpostal codes found\".format(sum(valid_postcode.values())))\n",
    "print(valid_postcode.most_common())\n",
    "\n",
    "print('-'*30)\n",
    "\n",
    "print(\"{} invalid postal codes were found\".format(len(invalid_postcode)))\n",
    "print(invalid_postcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct and Convert to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Function for Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_node(element):\n",
    "    \"\"\"Format Node object and associated tags.  Remove attributes as they are processed.\"\"\"\n",
    "    node = {'type':'node'}\n",
    "    #Extract raw information for this node\n",
    "    attributes = {a:element.attrib[a] for a in element.attrib}\n",
    "    tags = [(t.attrib['k'],t.attrib['v']) for t in element.findall('tag')]\n",
    "    #tags used a list of tuples instead of dict because of potential duplicate keys\n",
    "    \n",
    "    #Add ID\n",
    "    node['id'] = element.attrib['id']\n",
    "    del attributes['id']\n",
    "    \n",
    "    #Add Position\n",
    "    if 'lat' in attributes and 'lon' in attributes:\n",
    "        node['pos'] = [float(element.attrib[\"lat\"]), float(element.attrib[\"lon\"])]\n",
    "        del attributes['lat']\n",
    "        del attributes['lon']\n",
    "\n",
    "    #Created\n",
    "    CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "    created_attrib = [a for a in attributes if a in CREATED] #Which of the created attr are present\n",
    "    if len(created_attrib) > 0:\n",
    "        node['created'] = {}\n",
    "        for a in created_attrib:\n",
    "            node['created'][a] = element.attrib[a]\n",
    "            del attributes[a]\n",
    "    \n",
    "    #Remaining Attributes\n",
    "    for a in attributes:\n",
    "        if problemchars.match(a): #skip attributes with problematic keys\n",
    "            continue\n",
    "        else:\n",
    "            node[a] = element.attrib[a]\n",
    "\n",
    "    #Process tags\n",
    "    problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "    address = re.compile(r'^addr:([a-z]|_)+$')#match 'addr', one colon, some tag\n",
    "    gnis = re.compile(r'^gnis:([a-z]|_)+$')#similar structure to address.  Seems interesting.\n",
    "    lower = re.compile(r'^([a-z]|_)*$')\n",
    "    lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "\n",
    "    for k,v in tags:\n",
    "        if problemchars.match(k):\n",
    "            continue\n",
    "        elif address.match(k):\n",
    "            #Add Address dict if needed\n",
    "            if 'address' not in node.keys(): #add address dict if not present\n",
    "                node['address'] = dict()\n",
    "            #Get subnode and corrected (if possible) value (see above)\n",
    "            subnode = k.split(':')[1]\n",
    "            if subnode == 'street':\n",
    "                v = correct_streetname(v)\n",
    "            elif subnode == 'postcode':\n",
    "                v = correct_postcode(v)\n",
    "            #Add value\n",
    "            node['address'][subnode] = v\n",
    "        elif gnis.match(k):\n",
    "            if 'gnis' not in node.keys(): #add address dict if not present\n",
    "                node['gnis'] = dict()\n",
    "            node['gnis'][k.split(':')[1]] = v\n",
    "        elif lower.match(k) or lower_colon.match(k):\n",
    "            node[k] = v         \n",
    "    \n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting function for Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_way(element):\n",
    "    \"\"\"Format Way object and associated nd and tags\"\"\"\n",
    "    way = {'type':'way'}\n",
    "    \n",
    "    #Extract raw information for this node\n",
    "    attributes = {a:element.attrib[a] for a in element.attrib}\n",
    "    noderefs = [n.attrib['ref'] for n in element.findall('nd')]\n",
    "    tags = [(t.attrib['k'],t.attrib['v']) for t in element.findall('tag')]\n",
    "    \n",
    "    #Add ID\n",
    "    way['id'] = element.attrib['id']\n",
    "    del attributes['id']\n",
    "    \n",
    "    #Created\n",
    "    CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "    created_attrib = [a for a in attributes if a in CREATED] #Which of the created attr are present\n",
    "    if len(created_attrib) > 0:\n",
    "        way['created'] = {}\n",
    "        for a in created_attrib:\n",
    "            way['created'][a] = element.attrib[a]\n",
    "            del attributes[a]\n",
    "            \n",
    "    #Remaining Attributes\n",
    "    for a in attributes:\n",
    "        if problemchars.match(a): #skip attributes with problematic keys\n",
    "            continue\n",
    "        else:\n",
    "            way[a] = element.attrib[a]\n",
    "            \n",
    "    #Node Refs\n",
    "    if len(noderefs) > 0:\n",
    "        way['node_refs'] = noderefs\n",
    "    \n",
    "    #Process tags\n",
    "    problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "    tiger = re.compile(r'^tiger:([a-z]|_)+$') #Import from US Census Data for roadways, etc\n",
    "    lower = re.compile(r'^([a-z]|_)*$')\n",
    "    lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "\n",
    "    for k,v in tags:\n",
    "        if problemchars.match(k):\n",
    "            continue\n",
    "        elif tiger.match(k):\n",
    "            if 'tiger' not in way.keys(): #add address dict if not present\n",
    "                way['tiger'] = dict()\n",
    "            way['tiger'][k.split(':')[1]] = v\n",
    "        elif lower.match(k) or lower_colon.match(k):\n",
    "            way[k] = v \n",
    "    \n",
    "    return way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    if element.tag == \"node\":\n",
    "        shaped_element = process_node(element)\n",
    "    elif element.tag == 'way':\n",
    "        shaped_element = process_way(element)\n",
    "    else:\n",
    "        shaped_element = None\n",
    "    return shaped_element\n",
    "        \n",
    "\n",
    "def process_map(file_in):\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    with open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                fo.write(json.dumps(el)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_map(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-07-08T00:16:44.580-0400\tconnected to: localhost\n",
      "2015-07-08T00:16:44.580-0400\tdropping: osm.raleigh\n",
      "2015-07-08T00:16:47.551-0400\t[#.......................] osm.raleigh\t25.3 MB/562.6 MB (4.5%)\n",
      "2015-07-08T00:16:50.551-0400\t[##......................] osm.raleigh\t53.7 MB/562.6 MB (9.5%)\n",
      "2015-07-08T00:16:53.552-0400\t[###.....................] osm.raleigh\t82.6 MB/562.6 MB (14.7%)\n",
      "2015-07-08T00:16:56.551-0400\t[####....................] osm.raleigh\t110.3 MB/562.6 MB (19.6%)\n",
      "2015-07-08T00:16:59.551-0400\t[#####...................] osm.raleigh\t138.2 MB/562.6 MB (24.6%)\n",
      "2015-07-08T00:17:02.551-0400\t[#######.................] osm.raleigh\t165.4 MB/562.6 MB (29.4%)\n",
      "2015-07-08T00:17:05.551-0400\t[########................] osm.raleigh\t193.3 MB/562.6 MB (34.4%)\n",
      "2015-07-08T00:17:08.552-0400\t[#########...............] osm.raleigh\t221.8 MB/562.6 MB (39.4%)\n",
      "2015-07-08T00:17:11.551-0400\t[##########..............] osm.raleigh\t245.6 MB/562.6 MB (43.7%)\n",
      "2015-07-08T00:17:14.551-0400\t[###########.............] osm.raleigh\t268.0 MB/562.6 MB (47.6%)\n",
      "2015-07-08T00:17:17.551-0400\t[############............] osm.raleigh\t294.1 MB/562.6 MB (52.3%)\n",
      "2015-07-08T00:17:20.552-0400\t[#############...........] osm.raleigh\t322.4 MB/562.6 MB (57.3%)\n",
      "2015-07-08T00:17:23.551-0400\t[##############..........] osm.raleigh\t350.7 MB/562.6 MB (62.3%)\n",
      "2015-07-08T00:17:26.551-0400\t[################........] osm.raleigh\t379.4 MB/562.6 MB (67.4%)\n",
      "2015-07-08T00:17:29.552-0400\t[#################.......] osm.raleigh\t408.3 MB/562.6 MB (72.6%)\n",
      "2015-07-08T00:17:32.551-0400\t[##################......] osm.raleigh\t436.9 MB/562.6 MB (77.7%)\n",
      "2015-07-08T00:17:35.551-0400\t[###################.....] osm.raleigh\t456.4 MB/562.6 MB (81.1%)\n",
      "2015-07-08T00:17:38.551-0400\t[####################....] osm.raleigh\t488.0 MB/562.6 MB (86.7%)\n",
      "2015-07-08T00:17:41.551-0400\t[######################..] osm.raleigh\t524.8 MB/562.6 MB (93.3%)\n",
      "2015-07-08T00:17:44.551-0400\t[#######################.] osm.raleigh\t560.6 MB/562.6 MB (99.6%)\n",
      "2015-07-08T00:17:44.753-0400\timported 2735730 documents\n"
     ]
    }
   ],
   "source": [
    "#Using mongoimport\n",
    "!mongoimport -d osm -c raleigh --drop --file=\"../raleigh_north-carolina.osm.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Auditing of the imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.osm\n",
    "collection = db.raleigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2735730"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of documents\n",
    "collection.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education amenity tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 3, '_id': 'Duke University East Campus'}\n",
      "{'count': 1, '_id': 'Duke University Central Campus'}\n",
      "{'count': 1, '_id': 'North Carolina State University (Centennial Campus)'}\n",
      "{'count': 1, '_id': None}\n",
      "{'count': 1, '_id': 'Duke University West Campus'}\n",
      "{'count': 1, '_id': \"St. Augustine's University\"}\n",
      "{'count': 1, '_id': 'Duke University Medical Center'}\n",
      "{'count': 1, '_id': 'JC Raulston Arboretum at NC State University'}\n",
      "{'count': 1, '_id': 'Campbell University: Norman Adrian Wiggins School of Law'}\n",
      "{'count': 1, '_id': 'North Carolina Central University'}\n"
     ]
    }
   ],
   "source": [
    "#List of university\n",
    "pipeline = [\n",
    "    {'$match':{'amenity':'university'}},\n",
    "    {'$group':{'_id':'$name', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$limit':10}\n",
    "]\n",
    "documents = collection.aggregate(pipeline)\n",
    "for r in documents['result']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 28, '_id': None}\n",
      "{'count': 1, '_id': 'Durham Tech Community College'}\n",
      "{'count': 1, '_id': 'Collins Building'}\n",
      "{'count': 1, '_id': 'White Building'}\n",
      "{'count': 1, '_id': 'Wake Technical Community College: Perry Health Sciences Campus'}\n",
      "{'count': 1, '_id': 'Meredith College'}\n",
      "{'count': 1, '_id': 'AKG Guitar Lessons'}\n"
     ]
    }
   ],
   "source": [
    "#List of college\n",
    "pipeline = [\n",
    "    {'$match':{'amenity':'college'}},\n",
    "    {'$group':{'_id':'$name', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$limit':10}\n",
    "]\n",
    "documents = collection.aggregate(pipeline)\n",
    "for r in documents['result']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 21, '_id': None}\n",
      "{'count': 2, '_id': 'Carrboro Elementary School'}\n",
      "{'count': 2, '_id': 'Durham Academy'}\n",
      "{'count': 2, '_id': 'Durham Technical Community College'}\n",
      "{'count': 2, '_id': 'Eastway Elementary School'}\n",
      "{'count': 2, '_id': \"Lowe's Grove Middle School\"}\n",
      "{'count': 2, '_id': 'The Goddard School'}\n",
      "{'count': 2, '_id': 'Ravenscroft School'}\n",
      "{'count': 2, '_id': 'Panther Creek High School'}\n",
      "{'count': 2, '_id': 'C C Spaulding Elementary School'}\n"
     ]
    }
   ],
   "source": [
    "#List of school\n",
    "pipeline = [\n",
    "    {'$match':{'amenity':'school'}},\n",
    "    {'$group':{'_id':'$name', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$limit':10}\n",
    "]\n",
    "documents = collection.aggregate(pipeline)\n",
    "for r in documents['result']:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Multiple points on the same position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ \n",
      " [35.8950081, -79.0633303] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '373215082', 'created': {'uid': '38487', 'version': '1', 'timestamp': '2009-04-12T14:28:50Z', 'user': 'jumbanho', 'changeset': '444838'}, 'pos': [35.8950081, -79.0633303], 'type': 'node', '_id': ObjectId('559ca43177608123d0c9f62e')}\n",
      "{'id': '373216876', 'created': {'uid': '38487', 'version': '1', 'timestamp': '2009-04-12T14:31:27Z', 'user': 'jumbanho', 'changeset': '444838'}, 'pos': [35.8950081, -79.0633303], 'type': 'node', '_id': ObjectId('559ca43177608123d0c9f6ac')}\n",
      "{'id': '373217228', 'created': {'uid': '38487', 'version': '1', 'timestamp': '2009-04-12T14:31:56Z', 'user': 'jumbanho', 'changeset': '444838'}, 'pos': [35.8950081, -79.0633303], 'type': 'node', '_id': ObjectId('559ca43177608123d0c9f6f0')}\n",
      "{'id': '373220087', 'created': {'uid': '38487', 'version': '1', 'timestamp': '2009-04-12T14:36:37Z', 'user': 'jumbanho', 'changeset': '444838'}, 'pos': [35.8950081, -79.0633303], 'type': 'node', '_id': ObjectId('559ca43177608123d0c9f8e5')}\n",
      "------------------------------ \n",
      " [35.8784081, -78.7838664] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '343671678', 'created': {'uid': '2294834', 'version': '5', 'timestamp': '2015-06-24T17:49:40Z', 'user': 'bigal945', 'changeset': '32188921'}, 'pos': [35.8784081, -78.7838664], 'type': 'node', '_id': ObjectId('559ca43077608123d0c98090')}\n",
      "{'id': '3615653571', 'created': {'uid': '2294834', 'version': '1', 'timestamp': '2015-06-24T17:49:40Z', 'user': 'bigal945', 'changeset': '32188921'}, 'pos': [35.8784081, -78.7838664], 'type': 'node', '_id': ObjectId('559ca46177608123d0ed3860')}\n",
      "{'id': '3615653572', 'created': {'uid': '2294834', 'version': '1', 'timestamp': '2015-06-24T17:49:40Z', 'user': 'bigal945', 'changeset': '32188921'}, 'pos': [35.8784081, -78.7838664], 'type': 'node', '_id': ObjectId('559ca46177608123d0ed3864')}\n",
      "{'id': '3615653573', 'created': {'uid': '2294834', 'version': '1', 'timestamp': '2015-06-24T17:49:40Z', 'user': 'bigal945', 'changeset': '32188921'}, 'pos': [35.8784081, -78.7838664], 'type': 'node', '_id': ObjectId('559ca46177608123d0ed386a')}\n",
      "------------------------------ \n",
      " [35.7877479, -78.8708045] \n",
      " ------------------------------ \n",
      "\n",
      "{'id': '195496249', 'created': {'uid': '1494110', 'version': '7', 'timestamp': '2013-10-04T18:14:14Z', 'user': 'KristenK', 'changeset': '18183244'}, 'pos': [35.7877479, -78.8708045], 'type': 'node', '_id': ObjectId('559ca42f77608123d0c881ec')}\n",
      "{'id': '2482640856', 'created': {'uid': '1494110', 'version': '1', 'timestamp': '2013-10-04T18:13:55Z', 'user': 'KristenK', 'changeset': '18183244'}, 'pos': [35.7877479, -78.8708045], 'type': 'node', '_id': ObjectId('559ca45f77608123d0eb92ee')}\n",
      "{'id': '2482640858', 'created': {'uid': '1494110', 'version': '1', 'timestamp': '2013-10-04T18:13:55Z', 'user': 'KristenK', 'changeset': '18183244'}, 'pos': [35.7877479, -78.8708045], 'type': 'node', '_id': ObjectId('559ca45f77608123d0eb92ef')}\n",
      "{'id': '2482640859', 'created': {'uid': '1494110', 'version': '1', 'timestamp': '2013-10-04T18:13:55Z', 'user': 'KristenK', 'changeset': '18183244'}, 'pos': [35.7877479, -78.8708045], 'type': 'node', '_id': ObjectId('559ca45f77608123d0eb92f0')}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {'$group':{'_id':'$pos', 'count':{'$sum':1}}},\n",
    "    {'$sort':{'count':-1}},\n",
    "    {'$skip':1}, #skip \"None\" record\n",
    "    {'$limit':3}\n",
    "]\n",
    "documents = collection.aggregate(pipeline, allowDiskUse=True)\n",
    "\n",
    "for r in documents['result']:\n",
    "    pos = r['_id']\n",
    "    print(\"-\"*30,\"\\n\",pos,\"\\n\",\"-\"*30,\"\\n\")\n",
    "    overlaps = collection.aggregate({'$match':{'pos':pos}})\n",
    "    for o in overlaps['result']:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
